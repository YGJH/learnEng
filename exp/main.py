from fastapi import FastAPI, HTTPException
import feedparser
from typing import List, Optional
import datetime
import json
import os
from pathlib import Path

app = FastAPI(title="My No-Key News API", description="A custom RSS-based News API")

# å®šç¾© JSON æ–‡ä»¶è·¯å¾‘
NEWS_JSON_FILE = Path(__file__).parent / "news.json"

# å®šç¾©æˆ‘å€‘çš„æ–°èä¾†æº (ä½ å¯ä»¥éš¨æ„å¢åŠ )
RSS_FEEDS = {
    "technology": [
        "https://www.wired.com/feed/rss",
        "http://feeds.bbci.co.uk/news/technology/rss.xml",
        "https://techcrunch.com/feed/",
        "https://www.theverge.com/rss/index.xml"
    ],
    "business": [
        "http://feeds.bbci.co.uk/news/business/rss.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/Business.xml",
        "https://www.cnbc.com/id/10000664/device/rss/rss.html",
        "https://feeds.a.dj.com/rss/WSJcomUSBusiness.xml"
    ],
    "world": [
        "http://feeds.bbci.co.uk/news/world/rss.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
        "https://www.aljazeera.com/xml/rss/all.xml",
        "http://rss.cnn.com/rss/edition_world.rss"
    ],
    "science": [
        "https://www.sciencedaily.com/rss/top_news.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/Science.xml",
        "https://www.newscientist.com/feed/home"
    ],
    "health": [
        "http://feeds.bbci.co.uk/news/health/rss.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/Health.xml",
        "https://www.medicalnewstoday.com/feed"
    ],
    "sports": [
        "http://feeds.bbci.co.uk/sport/rss.xml",
        "https://www.espn.com/espn/rss/news",
        "https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml"
    ],
    "entertainment": [
        "https://www.variety.com/feed/",
        "https://rss.nytimes.com/services/xml/rss/nyt/Movies.xml",
        "https://www.hollywoodreporter.com/feed/"
    ],
    "general": [
        "http://feeds.bbci.co.uk/news/rss.xml",
        "https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml",
        "http://rss.cnn.com/rss/edition.rss"
    ]
}

def parse_feed(url: str, category: str):
    """è§£æå–®å€‹ RSS URL ä¸¦æ¨™æº–åŒ–æ ¼å¼"""
    news_items = []
    feed = feedparser.parse(url)
    
    for entry in feed.entries:
        # å˜—è©¦ç²å–ç™¼å¸ƒæ™‚é–“ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ç•¶å‰æ™‚é–“
        pub_date = getattr(entry, 'published', str(datetime.datetime.now()))
        
        item = {
            "title": entry.title,
            "link": entry.link,
            "summary": getattr(entry, 'summary', 'No summary available'),
            "source": feed.feed.get('title', 'Unknown Source'),
            "published_at": pub_date,
            "category": category
        }
        news_items.append(item)
    return news_items

def save_news_to_json(news_data: dict):
    """å°‡æ–°èæ•¸æ“šä¿å­˜åˆ° JSON æ–‡ä»¶"""
    try:
        # æ·»åŠ æ™‚é–“æˆ³
        news_data["fetched_at"] = datetime.datetime.now().isoformat()
        
        with open(NEWS_JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… News saved to {NEWS_JSON_FILE}")
        print(f"ğŸ“Š Total articles: {news_data['count']}")
    except Exception as e:
        print(f"âŒ Error saving news to JSON: {e}")

def load_news_from_json():
    """å¾ JSON æ–‡ä»¶åŠ è¼‰æ–°èæ•¸æ“š"""
    if NEWS_JSON_FILE.exists():
        try:
            with open(NEWS_JSON_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ğŸ“° Loaded {data.get('count', 0)} articles from cache")
            return data
        except Exception as e:
            print(f"âš ï¸ Error loading news from JSON: {e}")
            return None
    return None

@app.get("/")
def read_root():
    return {
        "message": "Welcome to the Free News API. Use /news to get articles.",
        "cache_file": str(NEWS_JSON_FILE),
        "cache_exists": NEWS_JSON_FILE.exists()
    }

@app.get("/news/cached")
def get_cached_news():
    """
    å¾æœ¬åœ° JSON æ–‡ä»¶è®€å–ç·©å­˜çš„æ–°è
    """
    cached_data = load_news_from_json()
    if cached_data:
        return cached_data
    else:
        raise HTTPException(
            status_code=404,
            detail="No cached news found. Please call /news to fetch fresh data."
        )

#@app.get("/news")
def get_all_news(category: Optional[str] = None, limit: int = 100, use_cache: bool = False):
    """
    ç²å–æ–°èçš„ Endpoint
    - category: technology, business, world, general (å¦‚æœä¸æŒ‡å®šå‰‡æŠ“å–æ‰€æœ‰é¡åˆ¥)
    - limit: é™åˆ¶å›å‚³ç¯‡æ•¸ (é è¨­ 50 ç¯‡)
    - use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜ï¼ˆé è¨­ Falseï¼Œæœƒé‡æ–°æŠ“å–ï¼‰
    """
    # å¦‚æœä½¿ç”¨ç·©å­˜ä¸”ç·©å­˜å­˜åœ¨
    if use_cache:
        cached_data = load_news_from_json()
        if cached_data:
            # æ ¹æ“šè«‹æ±‚çš„ category å’Œ limit éæ¿¾ç·©å­˜æ•¸æ“š
            if category and category != "all" and cached_data.get("category") == "all":
                filtered_articles = [
                    a for a in cached_data.get("articles", [])
                    if a.get("category") == category
                ][:limit]
                return {
                    "count": len(filtered_articles),
                    "category": category,
                    "articles": filtered_articles,
                    "from_cache": True
                }
            return {**cached_data, "from_cache": True}
    
    all_articles = []
    
    # å¦‚æœæ²’æœ‰æŒ‡å®š categoryï¼Œå°±æŠ“å–æ‰€æœ‰é¡åˆ¥
    if category is None or category == "all":
        categories_to_fetch = RSS_FEEDS.keys()
        mixed_category = "all"
    else:
        if category not in RSS_FEEDS:
            raise HTTPException(
                status_code=404, 
                detail=f"Category '{category}' not found. Available: {list(RSS_FEEDS.keys())}"
            )
        categories_to_fetch = [category]
        mixed_category = category
    
    # æŠ“å–æ‰€æœ‰æŒ‡å®šé¡åˆ¥çš„ RSS ä¾†æº
    for cat in categories_to_fetch:
        urls = RSS_FEEDS[cat]
        for url in urls:
            try:
                articles = parse_feed(url, cat)
                all_articles.extend(articles)
            except Exception as e:
                print(f"Error parsing {url}: {e}")
                continue

    # æ ¹æ“šç™¼å¸ƒæ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
    try:
        all_articles.sort(
            key=lambda x: datetime.datetime.strptime(
                x['published_at'], 
                "%a, %d %b %Y %H:%M:%S %z"
            ) if x['published_at'] else datetime.datetime.min,
            reverse=True
        )
    except Exception as e:
        print(f"Sorting error: {e}")
        # å¦‚æœæ’åºå¤±æ•—ï¼Œå°±ä¿æŒåŸé †åº
        pass

    # é™åˆ¶æ•¸é‡
    limited_articles = all_articles[:limit]
    
    # æº–å‚™è¿”å›çš„æ•¸æ“š
    response_data = {
        "count": len(limited_articles),
        "category": mixed_category,
        "articles": limited_articles
    }
    
    # ä¿å­˜åˆ° JSON æ–‡ä»¶
    save_news_to_json(response_data)
    
    return response_data

if __name__ == "__main__":
    # æ–¹ä¾¿ç›´æ¥ç”¨ python news_api.py åŸ·è¡Œ
  #  import uvicorn
    get_all_news()
 #   uvicorn.run(app, host="0.0.0.0", port=9902)
